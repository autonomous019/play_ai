{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2v2GI3tovok"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning, model-based and model-free learning are two approaches for learning optimal policies from interactions with an environment.\n",
        "\n",
        "Model-based learning involves building a model of the environment that captures the dynamics of the state transitions and rewards. The agent uses this model to simulate future trajectories and evaluate potential actions before taking them. In other words, the agent learns the optimal policy by planning ahead using its model of the environment. This approach can be more sample-efficient than model-free learning because the agent can use its model to learn from simulated experiences before interacting with the environment.\n",
        "\n",
        "On the other hand, model-free learning does not involve building an explicit model of the environment. Instead, the agent learns the optimal policy by directly estimating the value of each state or state-action pair through trial-and-error interactions with the environment. This approach involves updating the agent's value estimates based on the observed rewards and next states without using a model to simulate future trajectories. Model-free learning is generally simpler and more scalable than model-based learning, but may require more interactions with the environment to learn an optimal policy.\n",
        "\n",
        "Overall, the choice between model-based and model-free learning depends on the specifics of the problem at hand, including the size of the state and action spaces, the complexity of the dynamics and rewards, and the available computational resources.\n"
      ],
      "metadata": {
        "id": "uGqIibkio4Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RL Algo with a Model**"
      ],
      "metadata": {
        "id": "2wdW7ZrHpV-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Define the environment\n",
        "n_states = 5\n",
        "n_actions = 2\n",
        "transition_prob = torch.tensor([\n",
        "    [0.7, 0.3],\n",
        "    [0.4, 0.6],\n",
        "    [0.2, 0.8],\n",
        "    [0.1, 0.9],\n",
        "    [0.5, 0.5],\n",
        "])\n",
        "rewards = torch.tensor([-0.1, -0.2, -0.3, -0.4, 1.0])\n",
        "\n",
        "# Define the model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_states, 10)\n",
        "        self.fc2 = nn.Linear(10, n_states * n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x.view(-1, n_states, n_actions)\n",
        "\n",
        "model = Model()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "n_episodes = 1000\n",
        "for i in range(n_episodes):\n",
        "    state = random.randint(0, n_states - 1)\n",
        "    history = []\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Generate an action from the model's belief of the environment\n",
        "        action_probs = model(torch.eye(n_states)[state])\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        # Take the action and observe the next state and reward\n",
        "        next_state = torch.multinomial(transition_prob[state], 1).item()\n",
        "        reward = rewards[next_state]\n",
        "        \n",
        "        # Update the model's belief of the environment based on the observed transition\n",
        "        target = reward + 0.9 * torch.max(model(torch.eye(n_states)[next_state]))\n",
        "        loss = nn.MSELoss()(model(torch.eye(n_states)[state])[0][action], target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update the current state\n",
        "        state = next_state\n",
        "        \n",
        "        # Check if the episode is done\n",
        "        if reward > 0:\n",
        "            done = True\n",
        "            print(\"Episode {} completed in {} steps\".format(i, len(history)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "4K4bhj7Ooycc",
        "outputId": "f65cacae-8c4a-40ea-d7bd-bba819ce66de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c63ea3f9d4d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Generate an action from the model's belief of the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Take the action and observe the next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S02qL20TpCwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a simple environment with 5 states, 2 actions, and transition probabilities and rewards represented as tensors. We then define a neural network model that takes a one-hot encoded state vector as input and outputs a tensor of action probabilities for each state. We use the model to generate actions and update its belief of the environment based on observed transitions. The model is trained using a mean squared error loss and an Adam optimizer. Finally, we run multiple episodes and print the number of steps taken to reach a positive reward.\n",
        "\n"
      ],
      "metadata": {
        "id": "PCGd4VsRpCJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model-Free RL Algo**"
      ],
      "metadata": {
        "id": "vBRBU4Q-pQHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Define the environment\n",
        "n_states = 5\n",
        "n_actions = 2\n",
        "transition_prob = torch.tensor([\n",
        "    [0.7, 0.3],\n",
        "    [0.4, 0.6],\n",
        "    [0.2, 0.8],\n",
        "    [0.1, 0.9],\n",
        "    [0.5, 0.5],\n",
        "])\n",
        "rewards = torch.tensor([-0.1, -0.2, -0.3, -0.4, 1.0])\n",
        "\n",
        "# Define the Q-function\n",
        "Q = torch.zeros(n_states, n_actions)\n",
        "\n",
        "# Set the learning rate and discount factor\n",
        "lr = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "# Train the Q-function\n",
        "n_episodes = 1000\n",
        "for i in range(n_episodes):\n",
        "    state = random.randint(0, n_states - 1)\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Choose an action using an epsilon-greedy policy\n",
        "        if random.random() < 0.1:\n",
        "            action = random.randint(0, n_actions - 1)\n",
        "        else:\n",
        "            action = torch.argmax(Q[state]).item()\n",
        "        \n",
        "        # Take the action and observe the next state and reward\n",
        "        next_state = torch.multinomial(transition_prob[state], 1).item()\n",
        "        reward = rewards[next_state]\n",
        "        \n",
        "        # Update the Q-function using the Q-learning algorithm\n",
        "        td_error = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
        "        Q[state][action] += lr * td_error\n",
        "        \n",
        "        # Update the current state\n",
        "        state = next_state\n",
        "        \n",
        "        # Check if the episode is done\n",
        "        if reward > 0:\n",
        "            done = True\n",
        "            print(\"Episode {} completed in {} steps\".format(i, len(history)))\n"
      ],
      "metadata": {
        "id": "XaTTmsi3oySM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a simple environment with 5 states, 2 actions, and transition probabilities and rewards represented as tensors. We then define a Q-function as a tensor of state-action values and use the Q-learning algorithm to update the Q-function based on observed transitions. The Q-function is updated using a learning rate and discount factor, and actions are chosen using an epsilon-greedy policy. Finally, we run multiple episodes and print the number of steps taken to reach a positive reward."
      ],
      "metadata": {
        "id": "UzLf5Os1pNsk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQu9mw1moyOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vl4msL0hpBkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DAMMwSBoyJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoE_LkMdox-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}