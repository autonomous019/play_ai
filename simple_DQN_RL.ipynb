{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z7zpWWli57O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s18aeBnXi7_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_CfY8zqi78E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = 2\n",
        "action_dim = 1"
      ],
      "metadata": {
        "id": "cev66KgBi74n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.memory = np.zeros((capacity, state_dim + action_dim + 1 + state_dim), dtype=np.float32)\n",
        "        self.position = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        transition = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "        self.memory[self.position] = np.concatenate((state, [action, reward], next_state))\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(len(self.buffer)), batch_size)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for index in indices:\n",
        "            state, action, reward, next_state, done = self.buffer[index]\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(done)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V1iViHn7i71N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, the replay buffer is initialized with a capacity, and the memory is allocated with zeros to store the transitions in the form of (state, action, reward, next_state, done). Whenever a new transition is added to the replay buffer, it is appended to the buffer and the corresponding entry in the memory is updated. When the replay buffer is full, the new entries overwrite the oldest ones, creating a circular buffer.\n",
        "\n",
        "The sample method is used to retrieve a batch of transitions from the replay buffer. It randomly selects batch_size transitions and returns the states, actions, rewards, next_states, and dones in separate lists.\n",
        "\n",
        "Note that in this implementation, the state_dim and action_dim variables are assumed to be defined elsewhere. Also, the np module from NumPy is assumed to be imported.\n"
      ],
      "metadata": {
        "id": "TNtY5fe2jBjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Define the Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.linear1(state))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "# Define the DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, hidden_dim, lr, gamma, epsilon):\n",
        "        self.env = env\n",
        "        self.q_net = QNetwork(env.observation_space.shape[0], env.action_space.n, hidden_dim)\n",
        "        self.target_q_net = QNetwork(env.observation_space.shape[0], env.action_space.n, hidden_dim)\n",
        "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_net(torch.FloatTensor(state))\n",
        "                return q_values.argmax().item()\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_q_values = self.target_q_net(torch.FloatTensor(next_states)).max(dim=1, keepdim=True)[0]\n",
        "            target_q_values = rewards + self.gamma * target_q_values * (1 - dones)\n",
        "\n",
        "        q_values = self.q_net(torch.FloatTensor(states)).gather(1, torch.LongTensor(actions))\n",
        "\n",
        "        loss = nn.functional.mse_loss(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self, num_episodes, batch_size):\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                if len(self.replay_buffer) >= batch_size:\n",
        "                    self.update(batch_size)\n",
        "            if episode % 10 == 0:\n",
        "                self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "id": "r-pIVBZLi7wh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Q-network and a DQN agent, and includes the main training loop for the agent. It assumes that the OpenAI Gym environment is already defined and initialized.\n",
        "\n",
        "Note that this code assumes the existence of a replay buffer class that contains the replay buffer data and sampling methods. The implementation of the replay buffer is omitted for brevity.\n",
        "\n",
        "Also, note that this is a relatively basic implementation of a DQN algorithm and may not be optimal for all problems."
      ],
      "metadata": {
        "id": "FOoKX0VZjXkp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAnF6O_yi7mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}